{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Perkenalan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latar Belakang\n",
    "\n",
    "Penyakit Kardiovaskular (CVDs) adalah kategori penyakit yang melibatkan sistem kardiovaskular, yang terdiri dari jantung dan pembuluh darah. Penyakit Kardiovaskular (CVDs), yang merupakan penyebab utama kematian di seluruh dunia. CVDs memakan korban sekitar 17,9 juta jiwa setiap tahun, atau sekitar 31% dari total kematian global. Salah satu dampak umum dari CVDs adalah kegagalan jantung, yang dapat menjadi penyebab kematian.\n",
    "\n",
    "Dalam menghadapi tantangan prediksi penyakit Kardiovaskular (CVDs) dan kegagalan jantung, pendekatan terbaik melibatkan konsep Classification dengan penggunaan ensemble model. Tahap awal melibatkan persiapan data yang seksama, termasuk pengolahan dan pembersihan data, pembagian dataset, dan transformasi data yang diperlukan. Dengan data yang siap, implementasi ensemble model, seperti Random Forest atau Gradient Boosting, dijalankan untuk melakukan prediksi yang akurat terkait risiko dan kemungkinan munculnya penyakit kardiovaskular pada individu tertentu. Pemilihan ensemble model yang cermat dan pengoptimalkan parameter memainkan peran kunci dalam memastikan prediks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Analisis ini menggunakan data yang bersumber dari Kaggle yaitu <a href =\"https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data\" tittle = \"Heart Failure Prediction\">**Heart Failure Prediction**</a>. Kumpulan data ini berisi 299 baris dan 13 kolom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tujuan\n",
    "\n",
    "Project ini dibuat guna mengevaluasi konsep ensemble sebagai berikut:\n",
    "* Mampu memahami konsep Classification dengan Ensemble Model\n",
    "* Mampu mempersiapkan data untuk digunakan dalam Ensemble Model\n",
    "* Mampu mengimplementasikan Ensemble Model untuk membuat prediksi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ylyvvmWbaoJ"
   },
   "source": [
    "# B. Import Pustaka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "IZQgRjzHbaoK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "DusH_ua8baoM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVy2Ax8LbaoN"
   },
   "source": [
    "# C. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "g7wHvJijbaoN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'heart_failure_clinical_records_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheart_failure_clinical_records_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'heart_failure_clinical_records_dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('heart_failure_clinical_records_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yov3TNZ8baoO"
   },
   "source": [
    "> dari informasi data frame kita dapat melihat bahwa tidak ada nilai yang hilang pada dataset tersebut. dan tidak ada data kategorikal. setidaknya tidak dalam bentuk nilai numerik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSNK9FBnbaoP",
    "outputId": "d6ebafed-34cb-4e22-d2fa-f1823f8a6668",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb7lOkkUbaoQ"
   },
   "source": [
    "> dari ukuran tendensi sentral terlihat bahwa beberapa kolom mempunyai nilai yang sangat tinggi. tapi apakah itu sebuah hal yang aneh?\n",
    "> kurasa kita akan mengetahuinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "Ogv6OYnBbaoR",
    "outputId": "10fcf340-2d63-4b50-b501-a707bc596d5e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OhdaSTMbaoS"
   },
   "source": [
    "> mari kita lihat kardinalitas kolomnya. dan dari sini kami menemukan bahwa banyak kolom yang sebenarnya merupakan data kategorikal. dikodekan pada nilai biner. jadi kami akan menandainya sebagai data kategorikal. untuk menggunakannya dengan lebih baik di masa depan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3_0F-XxbaoT",
    "outputId": "423aa133-5941-4402-fbde-29e499e10e98",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(f'{col}'.ljust(24), f\"{len(df[col].unique())}\".rjust(8), 'Unique Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj_g0Wi7baoV"
   },
   "source": [
    "> ## Perbaikan Tipe\n",
    "> mari kita perbaiki tipe data yang dikodekan sebagai int ke data kategorikal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVKl985BbaoW",
    "outputId": "bff660df-122e-40ba-9483-54803fce99df",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "category_cols = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'DEATH_EVENT']\n",
    "\n",
    "for col in category_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_8jZQU8baoY"
   },
   "source": [
    "# D. Explorasi Data\n",
    "> jika kita menginginkan performa model yang baik, kita harus memulai dari tempat yang tepat. dan itu dimulai dengan memahami data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJHm3K6rbaoZ"
   },
   "source": [
    "> ## Numerical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJfrRoBXbaoa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def maximum_normalize_residual_test(sample, alpha=0.05, verbose=False):\n",
    "    n_sample = len(sample)\n",
    "    sample_mean = np.mean(sample)\n",
    "    sample_std = np.std(sample)\n",
    "\n",
    "    g_calculated = np.max(np.abs(sample - sample_mean)) / sample_std\n",
    "\n",
    "    t_value = stats.t.ppf(1 - (alpha / (2 * n_sample)), n_sample - 2)\n",
    "    g_crit = ((n_sample - 1) * np.sqrt(np.square(t_value)))\n",
    "    g_crit /= (np.sqrt(n_sample) * np.sqrt(n_sample - 2 + np.square(t_value)))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Grubbs Statistic: {g_calculated:.3f}')\n",
    "        print(f'Critical Value: {g_crit:.3f}')\n",
    "        if g_crit > g_calculated:\n",
    "            print(\"Accept Null Hypothesis, H0 : Sample doesn't contain outliers\")\n",
    "        else:\n",
    "            print(\"Reject Null Hypothesis, H1 : Sample contains outliers\")\n",
    "\n",
    "    return g_calculated > g_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKWtGY2rbaoc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def class_distribution(df, col, target, labels: dict):\n",
    "    _space = np.linspace(df[col].min(), df[col].max(), 100)\n",
    "\n",
    "    dists = {}\n",
    "    for c in labels:\n",
    "        dists[labels[c]] = stats.gaussian_kde(df[df[target] == c][col])(_space)\n",
    "\n",
    "    return _space, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WS5UcZxBbaod",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_distribution(df, col, target='DEATH_EVENT', labels=None):\n",
    "    df = df.copy()\n",
    "    labels = labels or {0: 'Negative', 1: 'Positive'}\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8 * 2, 6))\n",
    "\n",
    "    _space, _probs = class_distribution(df, col, target, labels)\n",
    "\n",
    "    for c, p in _probs.items():\n",
    "        ax[0].plot(_space, p)\n",
    "    ax[0].plot(_space, np.sum([_probs[c] for c in _probs], axis=0), 'g--')\n",
    "\n",
    "    ax[1].boxplot([df[df[target] == c][col] for c in labels])\n",
    "\n",
    "    ax[0].axvline(df[col].mean(), color='r', linestyle='--', label='Mean')\n",
    "    ax[0].axvline(df[col].median(), color='y', linestyle='--', label='Median')\n",
    "\n",
    "    ax[0].legend([label for label in labels.values()] + ['Histogram', 'Mean', 'Median'])\n",
    "    ax[0].set_title(f'{col.title()} Distribution')\n",
    "\n",
    "    maximum_normalize_residual_test(df[col], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBDgce1_baof"
   },
   "source": [
    ">> ### Age\n",
    ">> Dari histogram kita dapat melihat bahwa usia mempunyai korelasi dengan target kita, kita dapat mengatakan demikian karena ada perbedaan antara usia orang yang meninggal dan usia orang yang tidak meninggal. dalam hal ini kelompok usia 40-60 tahun memiliki angka kelas negatif yang lebih tinggi dibandingkan kelompok usia lebih dari 60 tahun. jadi dari fitur ini kita punya petunjuk tentang kemungkinan kematian mereka. dan tidak ada outlier yang terdeteksi pada fitur ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "Ck2-a9oLbaoh",
    "outputId": "d3414104-2870-446a-f5c2-c2cdc27efdb9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution(df, 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaThW1VHbaoj"
   },
   "source": [
    ">> ### Creatinine Phosphokinase\n",
    ">> Fitur yang kami amati ini memiliki distribusi yang hampir sama pada dua kelas. jadi kita dapat mengatakan bahwa kreatinin fosfokinase tidak ada korelasinya dengan target.\n",
    ">> tapi tetap saja saya akan memberikan beberapa perubahan pada fitur ini untuk melanjutkan ke pemilihan fitur. jadi untuk saat ini saya akan mengubahnya menjadi distribusi yang lebih normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "5GroThyvbaok",
    "outputId": "c66389ec-6976-41d5-cd93-7cb293431e7d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution(df, 'creatinine_phosphokinase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSEkkVX-baok"
   },
   "source": [
    ">> ### Ejection Fraction\n",
    ">> fitur ini mewakili fraksi ejeksi pasien. fitur ini mungkin memiliki perbedaan probabilistik antara kelas positif dan negatif.\n",
    ">> kelas positif cenderung memiliki fraksi ejeksi yang lebih rendah dibandingkan kelas negatif. jadi kita dapat mengatakan bahwa fraksi ejeksi mempunyai korelasi dengan target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "nGpVuQ9ubaol",
    "outputId": "677050cd-9f2a-46ca-c04c-e09f77529d80",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution(df, 'ejection_fraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njHn7PPJbaol"
   },
   "source": [
    ">> ### Platelets\n",
    ">> dari pandangan pertama fitur ini merupakan distribusi normal tetapi setelah dilakukan pemisahan kelas kita dapat melihat bahwa distribusinya tumpang tindih. dan itu tidak bagus untuk model kita. karena kita membutuhkan kelas yang didistribusikan secara berbeda dan tidak terlalu banyak tumpang tindih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "vH92pc9rbaom",
    "outputId": "fc7e48c6-cfd5-4bf0-db00-6330fe06e254",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution(df, 'platelets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnOAt4PVbaom"
   },
   "source": [
    ">> ### Serum Creatinine\n",
    ">> Fitur ini tentang , dari visualisasi kita dapat melihat bahwa distribusi kelas memiliki beberapa tumpang tindih tetapi memiliki mean yang berbeda dan di beberapa wilayah mereka tidak terlalu tumpang tindih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "s_cYckK6baon",
    "outputId": "40bf3f86-84c8-4fb9-d5ed-b4dbff8e8c55",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution(df, 'serum_creatinine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX7Z395Xbaon"
   },
   "source": [
    ">> ### Serum Sodium\n",
    ">> dari visualisasi terlihat bahwa fitur `serum_sodium` mempunyai sebaran yang cukup baik pada kelas sasaran, dapat dikatakan demikian karena hanya ada satu puncak sebaran untuk setiap kelas, dan rata-ratanya berbeda. jadi kami dapat menyimpulkan bahwa fitur ini adalah salah satu fitur terpenting untuk model kami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "XRA-d7-Dbaoo",
    "outputId": "f7624308-8847-4f78-ed3d-b841efc02c99",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution(df, 'serum_sodium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNf8PGmXbaoq"
   },
   "source": [
    ">> ### Time\n",
    ">> Saya masih kurang paham tentang representasi kolom ini karena kolom ini mempunyai korelasi yang sangat tinggi dengan target namun bagaimana hal itu terjadi masih ada bagian yang hilang. tetapi jika saya menemukan fitur ini tidak mungkin diperoleh sebelum masa hidup pasien berakhir, saya harus menghilangkan fitur ini untuk membuat kumpulan data ini lebih realistis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "MZHCl9PXbaoq",
    "outputId": "073bfebe-7a08-4087-b5e4-aa2f11f3d31c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution(df, 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7H3uJzKbaor"
   },
   "source": [
    "> ## Categorical Features\n",
    "> Karena kumpulan data ini hanya berisi kategori biner, kami akan mengamati semuanya sekaligus untuk menghemat waktu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktu4984Jbaos",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_categorical_dist(df, columns: list, target='DEATH_EVENT'):\n",
    "    fig, ax = plt.subplots(1, len(columns), figsize=(8 * len(columns), 6))\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        ax[i].set_title(column)\n",
    "        ax[i].hist(df[df[target] == 0][column], bins=2, histtype='step', label='Negative', density=True)\n",
    "        ax[i].hist(df[df[target] == 1][column], bins=2, histtype='step', label='Positive', density=True)\n",
    "        ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "6WGGXlaYbaos",
    "outputId": "34f78ea2-88aa-4023-bf57-4f47c96eb2c3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_categorical_dist(df, ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sssAwQvQbaos"
   },
   "source": [
    ">> Dari plot kita dapat melihat bahwa kolom kategori yang kita miliki tidak memberikan korelasi yang baik terhadap target karena mereka melebihi probabilitas pada 2 kelas, tapi saya akan menilai apa yang penting dan bukan dari peringkat korelasi agar adil dan tidak bias pada sudut pandang saya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbQCqnKcbaot"
   },
   "source": [
    "> ## Feature Correlation\n",
    "> setelah kami menyelidiki pada tingkat individual fitur kami, kami akan mencoba menemukan korelasi antar variabel apakah variabel tersebut berisi informasi yang sama.\n",
    "> tetapi dari pengamatan kami saya tidak melihat fitur yang memiliki korelasi terlalu kuat dengan fitur lainnya hanya ada satu fitur yang memiliki korelasi 0,45 dengan fitur lainnya yaitu `seks` dan `merokok` tetapi masih belum meyakinkan saya bahwa keduanya tidak membawa informasi tambahan secara individual jadi dalam hal ini saya akan menyimpan kolom tersebut jika kolom tersebut memiliki korelasi yang cukup dengan target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jUalUCPebaou",
    "outputId": "e5cb1c2b-ec20-441c-93ba-be0b382b39f5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_df = df.copy()\n",
    "_df[_df.select_dtypes('category').columns] = _df[_df.select_dtypes('category').columns].apply(lambda col: col.cat.codes)\n",
    "_corr = _df.corr(method='spearman').abs()\n",
    "\n",
    "plt.figure(figsize=(8 * 2, 8 * 2))\n",
    "sns.heatmap(_corr, cmap='coolwarm', annot=True, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcVBES-0baov"
   },
   "source": [
    "> ## Feature Selection\n",
    "> di bagian ini saya akan menjabarkan fitur terpenting untuk melatih model kita agar model menjadi ramping dan tidak overfitting.\n",
    "> seperti yang kita lihat hanya ada 5 kolom yang memiliki korelasi wajar dengan target.\n",
    "> jadi saya akan menyimpan kolom berikut:\n",
    "> - ~~time~~\n",
    "> - serum_creatinine\n",
    "> - ejection_fraction\n",
    "> - age\n",
    "> - serum_sodium\n",
    "\n",
    "> **Catatan**:\n",
    "> Setelah beberapa informasi ditemukan saya akan menghilangkan kolom waktu karena tidak mungkin memperoleh informasi tersebut sebelum akhir hidup pasien atau pada waktu prediksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "IJS81ISlbaov",
    "outputId": "8b08210e-0ec8-428d-dc06-93def16a422a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_corr = _df.corr(method='spearman').abs()['DEATH_EVENT'].sort_values(ascending=False).drop('DEATH_EVENT')\n",
    "lower_corr_threshold = _corr.max() / 5\n",
    "\n",
    "plt.figure(figsize=(8 * 2, 6))\n",
    "\n",
    "_corr.plot.bar()\n",
    "plt.axhline(lower_corr_threshold, color='black', linestyle='--')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzOnr_Vvbaow"
   },
   "source": [
    "# E. Data Preprocessing\n",
    "> Setelah setiap fitur dianalisis, dan kami memutuskan fitur apa yang ingin kami pertahankan, kami akan mulai memproses data terlebih dahulu.\n",
    "> karena proyek ini mengharuskan kita menggunakan model dasar pohon, saya tidak akan melakukan standarisasi apa pun pada data karena tidak masuk akal untuk model pohon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0wUlkPUbaox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = ['serum_creatinine', 'ejection_fraction', 'age', 'serum_sodium']\n",
    "preprocessing_pipeline = ColumnTransformer([('feature', 'passthrough', selected_features)])\n",
    "\n",
    "create_pipeline = lambda model: Pipeline(\n",
    "    [\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('model', model)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOklfUn2baox"
   },
   "source": [
    "> ## Cross Validation Split\n",
    "> pada bagian ini kita akan membagi data kita menjadi 3 bagian dan bagian tersebut adalah pelatihan, validasi dan pengujian dan usahakan proporsi setiap bagian tetap sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxCwMAAtbaoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = df.drop(['DEATH_EVENT'], axis=1)\n",
    "y = df['DEATH_EVENT']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=seed, stratify=y)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vslCqEpUbaoz"
   },
   "source": [
    "> dari sini kita dapat melihat bahwa dataset yang kita bagi hanya berisi sedikit sampel di dalam bagian pelatihan, validasi, dan pengujian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4l8BYImMbao0",
    "outputId": "7453c734-3c11-431d-e283-779c8ff8a7c6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_val: {x_val.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qu9DhDsUbao1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_clv29sYbao2"
   },
   "source": [
    "# F. Pendefinisian Model\n",
    "> saatnya mendefinisikan model kita.\n",
    "> Saya akan mendefinisikan 2 model untuk dataset ini, yang satu adalah Random Forest dan yang lainnya adalah Gradient Boosting.\n",
    "\n",
    "> **Random Forest**\n",
    "> Random Forest atau Pohon keputusan acak adalah metode pembelajaran ansambel untuk klasifikasi, regresi, dan tugas-tugas lain yang beroperasi dengan membangun banyak pohon keputusan pada waktu pelatihan. Untuk tugas klasifikasi, keluaran dari hutan acak adalah kelas yang dipilih oleh sebagian besar pohon.\n",
    "\n",
    "> **Gradient Boosting**\n",
    "> Gradient boosting adalah teknik pembelajaran mesin yang digunakan antara lain dalam tugas regresi dan klasifikasi. Ini memberikan model prediksi dalam bentuk kumpulan model prediksi lemah, yang biasanya berupa pohon keputusan. Jika pohon keputusan merupakan pembelajar yang lemah, algoritma yang dihasilkan disebut pohon yang ditingkatkan gradien; biasanya kinerjanya melebihi hutan acak. Model pohon dengan peningkatan gradien dibangun secara bertahap seperti pada metode peningkatan lainnya, namun model ini menggeneralisasi metode lain dengan memungkinkan optimalisasi fungsi kerugian yang dapat dibedakan secara arbitrer.\n",
    "\n",
    "> karena kami hanya menggunakan 2 model, saya akan mencoba mencari parameter terbaik untuk setiap model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIT_j_4Nbao3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=7,\n",
    "    max_features='sqrt',\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "gb_model = create_pipeline(gb_model)\n",
    "rf_model = create_pipeline(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTfDP-1qbao3"
   },
   "source": [
    "# G. Pelatihan Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nknFMP24bao3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_hyper_parameter(model, param_grid, dataset):\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(*dataset)\n",
    "    return grid_search.best_estimator_, grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rPhPq9Lbao4"
   },
   "source": [
    "> mari kita mulai dengan penyetelan hyperparameter. di sini kami mendeklarasikan n_estimators dan max_ depth. untuk mendapatkan yang terbaik dari model kami dalam kumpulan data ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAZgu85ybao4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "gb_params = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03b1ohv4bao4",
    "outputId": "14875747-01d1-44af-8ae2-b7be7ae91c87",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rf_model, rf_scores = find_hyper_parameter(rf_model, rf_params, train_ds)\n",
    "gb_model, gb_scores = find_hyper_parameter(gb_model, gb_params, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcLsMumsbao5"
   },
   "source": [
    "> dan berikut adalah param terbaik yang didapat dari pencarian diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i71ZJvNxbao6",
    "outputId": "ccd68cd5-5faf-43a9-e7e2-449ae31823f9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rf_model[-1], gb_model[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeHw95iobao6"
   },
   "source": [
    "> ## Grid Search Score\n",
    "> dari metrik ini saja saya dapat mengatakan bahwa model tersebut berperforma baik tetapi serupa dengan model lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4TbYqnBbao7",
    "outputId": "d529c930-7ff6-40bd-f808-8a3d382b730d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Random Forest: Score - {np.mean(rf_scores['mean_test_score']):.3f}, Var - {np.mean(rf_scores['std_test_score']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDM-Motwbao7",
    "outputId": "dfa1e7fa-35b7-4db4-be19-79f5c635ea34",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Gradient Boost: Score - {np.mean(gb_scores['mean_test_score']):.3f}, Var - {np.mean(gb_scores['std_test_score']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ad42GjObao8"
   },
   "source": [
    "# H. Evaluasi Model\n",
    "> setelah semua dilatih saya akan mengevaluasi model dan melihat mana yang terbaik. dan terus menyempurnakan model untuk menyelesaikan tujuan kami.\n",
    "> dan saya akan menggunakan `mae` dan `mse` untuk mengukur kerugian setiap model dan menggunakan `acc`, `prec`, `recall` dan `f1` untuk mengukur kinerja model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QjiX9OQbao8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(model, dataset: (x, y), prefix, threshold=0.5):\n",
    "    x, y = dataset\n",
    "    _y = model.predict_proba(x)[:, 1]\n",
    "    _y = _y > threshold\n",
    "\n",
    "    acc = metrics.accuracy_score(y, _y)\n",
    "    f1 = metrics.f1_score(y, _y)\n",
    "    prec = metrics.precision_score(y, _y)\n",
    "    rec = metrics.recall_score(y, _y)\n",
    "\n",
    "    mse = metrics.mean_squared_error(y, _y)\n",
    "    mae = metrics.mean_absolute_error(y, _y)\n",
    "\n",
    "    print(f\"\"\"{prefix}\n",
    "    Accuracy : {acc:.2f}, F1 : {f1:.2f}, Precision : {prec:.2f}, Recall : {rec:.2f}\n",
    "    MSE : {mse:.2f}, MAE : {mae:.2f}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM0BgR8Wbao9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def display_roc(model, dataset: (x, y), name, ax=plt):\n",
    "    x, y = dataset\n",
    "    _y = model.predict_proba(x)[:, 1]\n",
    "    ax.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    metrics.RocCurveDisplay.from_predictions(y, _y, name=name, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aX1O1npkbao9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(model, dataset: (x, y), name, ax=plt):\n",
    "    x, y = dataset\n",
    "    _y = model.predict_proba(x)[:, 1]\n",
    "    ax.plot([0, 1], [1, 0], color='gray', linestyle='--')\n",
    "    metrics.PrecisionRecallDisplay.from_predictions(y, _y, name=name, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmCnhumKbao-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_space(model, dataset: (x, y), name):\n",
    "    x, y = dataset\n",
    "    _y = model.predict_proba(x)[:, 1]\n",
    "\n",
    "    reports = []\n",
    "\n",
    "    for t in np.arange(0, 1, .01):\n",
    "        output = _y > t\n",
    "\n",
    "        precision = metrics.precision_score(y, output, zero_division=0)\n",
    "        recall = metrics.recall_score(y, output, zero_division=0)\n",
    "\n",
    "        reports.append({\n",
    "            'treshold': t,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "        })\n",
    "\n",
    "    reports = pd.DataFrame(reports)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(reports['treshold'], reports['precision'], label='Precision')\n",
    "    plt.plot(reports['treshold'], reports['recall'], label='Recall')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7YdWqmmbao-"
   },
   "source": [
    "> ## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_hOI8Jybao_"
   },
   "source": [
    ">> ### Random Forest\n",
    ">> seperti yang kita lihat bahwa model ini berkinerja baik dan tidak melakukan overfitting pada data. dan ini merupakan indikasi bagus bahwa model tersebut akan berkinerja baik di masa depan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQFHQhubbao_",
    "outputId": "c1f6c984-f3cb-4beb-b25e-e1c6de02eb4a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculate_metrics(rf_model, dataset=train_ds, prefix='Random Forest - Train')\n",
    "calculate_metrics(rf_model, dataset=val_ds, prefix='Random Forest - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yohjswobapA"
   },
   "source": [
    ">> ### Gradient Boost\n",
    ">> Model ini berkinerja sangat baik dalam pelatihan tetapi dari apa yang kita lihat bahwa model tersebut tidak terlalu akurat pada kumpulan data validasi, hal ini karena masalah model basis pohon yang umum yaitu model tersebut selalu terlalu sesuai dengan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HO839nxbapA",
    "outputId": "6dfd9760-03f7-4d95-d41c-8bc7f8bb833c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculate_metrics(gb_model, dataset=train_ds, prefix='Gradient Boosting - Train')\n",
    "calculate_metrics(gb_model, dataset=val_ds, prefix='Gradient Boosting - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xoDf3NYbapB"
   },
   "source": [
    "> ## AUC-ROC and Precision-Recall Curve\n",
    "> setelah menilai performa model dengan angkanya saja kita akan melihat apakah pernyataan saya sebelumnya masih berlaku di Kurva ini.\n",
    "> seperti yang kita lihat kedua model hampir memiliki performa yang sama tetapi saya akan memilih model Random Forest karena modelnya lebih sederhana dan performa AUC-ROC lebih baik daripada Gradient Boosting.\n",
    "> jadi setelah ini saya hanya akan melanjutkan dengan model hutan acak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "cgwq0c3ZbapB",
    "outputId": "9e70e653-c58a-4eea-8ca3-09c78b1468bf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8 * 2, 6))\n",
    "\n",
    "display_roc(rf_model, dataset=val_ds, name='Random Forest - Validation', ax=ax[0])\n",
    "display_roc(gb_model, dataset=val_ds, name='Gradient Boosting - Validation', ax=ax[0])\n",
    "\n",
    "plot_precision_recall(rf_model, dataset=val_ds, name='Random Forest - Validation', ax=ax[1])\n",
    "plot_precision_recall(gb_model, dataset=val_ds, name='Gradient Boosting - Validation', ax=ax[1])\n",
    "\n",
    "ax[0].set_title('ROC Curve')\n",
    "ax[1].set_title('Precision Recall Curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiNuCd1dbapC"
   },
   "source": [
    "> ## Threshold Selection\n",
    "> oke setelah kita melihat bagaimana kinerja model kita pada metrik tersebut, kita memerlukan cara untuk mengimplementasikan model tersebut ke dalam domain masalah kita, jadi apa masalahnya? kita perlu menangkap sebanyak mungkin pasien kritis sebelum hal buruk terjadi pada mereka. jadi saya akan menukar presisi saya untuk penarikan kembali. dalam hal ini perolehan 80% akan bagus karena kinerja model pada AUC ROC sebelumnya\n",
    "\n",
    "> mari kita mulai dengan menggeser ambang batas dari 0 ke 1 dan menghitung perolehan kembali untuk setiap ambang batas. lalu pilih nilai ambang batas yang tepat untuk memaksimalkan perolehan kembali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "oIQLGV63bapC",
    "outputId": "0f6d474a-9e0f-4dac-e56a-b80ea82e8ad8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = plot_precision_recall_space(rf_model, dataset=val_ds, name='Random Forest - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4q4KeSIbapD"
   },
   "source": [
    "> oke setelah melihat kembali pilihan ambang batas kita dapat melihat bahwa ambang batas yang memaksimalkan penarikan kembali adalah 0,2 mengapa nilainya karena ketika kita menggambar garis lurus dari y=0 ke y=1 kita dapat melihat bahwa penarikan kembali adalah 1. dan presisinya adalah .33. jadi ambang batas yang memaksimalkan recall adalah 0,2. tapi apakah penilaianku benar? kita perlu memplot dan menghitung presisi dan perolehan dari ambang batas yang dipilih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ob7Qhr23bapD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "TueCJDj_bapD",
    "outputId": "663ed4be-c6f2-4fea-cb8f-6fb1bb7d2623",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_recall_space(rf_model, dataset=val_ds, name=f'Random Forest - Validation - Threshold {threshold}')\n",
    "plt.axvline(x=threshold, color='k', linestyle='--')\n",
    "plt.axhline(.94, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbn1CS8ubapD",
    "outputId": "f9d3dfd0-ca22-437a-f1a8-f9ddbfc3b50a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_y = rf_model.predict_proba(x_val)[:, 1] > threshold\n",
    "print(classification_report(y_val, _y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp9KLiNbbapG"
   },
   "source": [
    "> ## Conclusion\n",
    "> dari garis baru yang kita gambar dan dari perhitungan numerik terlihat bahwa asumsi kita sebelumnya benar. bahwa kita akan memiliki sekitar 94% recall dan 57% presisi. jadi mari kita mulai simulasi untuk melihat performa model kita di dunia nyata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWVD1iYAbapH"
   },
   "source": [
    "# I. Model Inference\n",
    "> di bagian ini kita akan melakukan simulasi di dunia nyata ketika kita menghadapi masalah di alam liar, katakanlah kita mempunyai 30 pasien dan kita perlu menangkap mereka yang memiliki risiko kematian lebih tinggi dalam waktu dekat karena masalah kebakaran. dan dari data simulasi kita dapat melihat mereka terdiri dari 20 pasien negatif dan 10 pasien positif. dan kita perlu menangkap pasien positif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsG1ilDHbapH",
    "outputId": "3d559e40-5182-43d1-86ea-643158e0cf54",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total Patient : {len(x_test)}\")\n",
    "print(f\"Negative Patient : {len(y_test[y_test == 0])}\")\n",
    "print(f\"Positive Patient : {len(y_test[y_test == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgWkSK0KbapI"
   },
   "source": [
    "> siapkan kumpulan data ke dalam format yang mudah dibaca untuk kami analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "id": "UtX2_VfLbapJ",
    "outputId": "c3800aa3-aa52-4c96-ccde-99169c1f6d41",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_x_test = x_test.copy()\n",
    "_x_test['actual'] = np.asarray(y_test, bool)\n",
    "_x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kbf5qfXhbapK"
   },
   "source": [
    "> melakukan prediksi probabilitas untuk masing-masing pasien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYHsFscqbapM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_x_test = x_test.copy()\n",
    "_x_test['prediction'] = rf_model.predict_proba(_x_test)[:, 1] > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgZYqYVTbapO"
   },
   "source": [
    "> pilih pasien yang mempunyai kemungkinan tertinggi untuk menjadi positif dan karantina mereka, sehingga kita memiliki peluang lebih besar untuk mengetahui kapan jantung mereka mulai gagal dan mengobatinya sebelum terlambat. seperti yang bisa kita lihat, model tersebut memperkirakan bahwa dari 30 pasien, hanya 15 pasien yang memiliki kemungkinan lebih tinggi untuk menjadi positif dan memerlukan observasi medis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "ffpJE3oIbapO",
    "outputId": "a061c0c2-b70e-41fc-bab8-8d5dfa4da771",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_x_test['actual'] = np.asarray(y_test, bool)\n",
    "_y_test = y_test[_x_test['prediction'] == True]\n",
    "_x_test = _x_test[_x_test['prediction'] == True]\n",
    "_x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Eeti3cfbapP"
   },
   "source": [
    "> oke, mari kita hitung kinerja model kita dengan kumpulan data simulasi\n",
    "> inilah yang kami dapatkan dari simulasi\n",
    "> - kita dapat mengidentifikasi 80% pasien positif dan 20% pasien salah klasifikasi. ini adalah peningkatan besar dalam perspektif akurasi ketika membandingkannya dengan peluang acak\n",
    "> - kita dapat melihat bahwa kita berhasil memilih jumlah pasien dalam periode observasi dan menghemat waktu dan uang operasi medis dengan hanya mengorbankan 20% pasien positif.\n",
    "\n",
    "> ringkasan :\n",
    "kami mendapatkan efisiensi hampir 200% untuk melanjutkan observasi medis dan merawat pasien yang tepat. dengan akurasi 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fiUSE9AWbapQ",
    "outputId": "5b4b0b08-95d1-408d-fd69-980426d32ace",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total Observation : {len(_y_test)}\")\n",
    "print(f\"Actual Negative Observation : {len(_y_test[(y_test == 0)])}\")\n",
    "print(f\"Actual Positive Observation : {len(_y_test[(y_test == 1)])}\")\n",
    "print()\n",
    "print(f\"Efficiency : {len(y_test) / len(_y_test) * 100 :.0f} %\")\n",
    "print(f\"Life Saved :  {len(_y_test[(y_test == 1)]) / len(y_test[y_test == 1]) * 100 :.0f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D54w_PwlbapQ"
   },
   "source": [
    "# J. Kesimpulan\n",
    "> dari perjalanan yang kita lalui sebelumnya kita dapat menyimpulkan banyak hal seperti dari EDA kita dapat mengatakan bahwa banyak kolom yang mungkin relevan dengan target di tingkat logika sebenarnya tidak berkorelasi dengan target, misalnya pada pemikiran pertama saya, saya Saya pikir akan ada korelasi besar antara hipertensi dan penyakit jantung. namun dari analisa statistik hal tersebut bukanlah faktanya karena kita tidak bisa membedakan keduanya hanya dari satu ciri saja. dan kolom waktu penting untuk dihilangkan karena tidak mungkin diperoleh waktu prediksi karena itu adalah waktu kematian pasien.\n",
    "\n",
    "> kemudian dari pemilihan model dan analisis kinerja kita dapat menyimpulkan sesuatu yang menarik bahwa model yang lebih maju tidak selalu merupakan model yang terbaik. seperti dalam kasus saya, saya menemukan bahwa pohon yang ditingkatkan yang kami terapkan tidak memiliki kinerja yang jauh lebih baik daripada model hutan acak. penyihir dalam hal arsitektur lebih sederhana dan lebih cepat untuk dihitung\n",
    "\n",
    "> pemilihan ambang batas memberi kita keuntungan bagus dalam tujuan ini dengan memperdagangkan beberapa presisi, kita bisa mendapatkan ingatan yang lebih baik untuk memenuhi tujuan kita.\n",
    "\n",
    "> dari simulasi saja kita dapat memberikan dampak yang besar pada 2 pemikiran pertama tentang efisiensi tenaga medis, sehingga mereka tidak perlu mempertahankan pasien berisiko rendah dalam periode observasi, dan mereka dapat menghemat waktu dan biaya operasi medis. penyihir dalam kasus simulasi saya mampu menangkap 8 dari 10 pasien positif sejati."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
